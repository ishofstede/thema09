---
title: "thema09_logboek"
author: "Isabella Hofstede"
date: "2023-09-13"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# week 1
For the purpose of this project a few packages and libraries will be installed. All of these are for the purpose of either plotting or formatting the data or plots.
Installing haven for formatting:
```{r echo = T, results = 'hide', warning=FALSE, message=FALSE}
#check if package is installed
if (!require(haven)){
  #if not, install it
  install.packages("haven", dependencies = TRUE)
  #if installed, load library
  library(haven)
}
```
Installing tidyverse for formatting:
```{r echo = T, results = 'hide', warning=FALSE,message=FALSE}
if (!require(tidyverse)){
  install.packages("tidyverse", dependencies = TRUE)
  library(tidyverse)
}
```
Installing dply for formatting:
```{r echo = T, results = 'hide', warning=FALSE, message=FALSE}
if (!require(dplyr)){
  install.packages("dplyr", dependencies = TRUE)
  library(dplyr)
}
```
Installing summarytools for formatting:
```{r echo = T, results = 'hide', warning=FALSE, message=FALSE}
if (!require(summarytools)){
  install.packages("summarytools", dependencies = TRUE)
  library(summarytools)
}
```
Installing ggplot2 for plotting:
```{r echo = T, results = 'hide', warning=FALSE, message=FALSE}
if (!require(ggplot2)){
  install.packages("ggplot2", dependencies = TRUE)
  library(ggplot2)
}
```
Installing GGally for plotting:
```{r echo = T, results = 'hide', warning=FALSE, message=FALSE}
if (!require(GGally)){
  install.packages("GGally", dependencies = TRUE)
  library(GGally)
}
```
Installing GridExtra for formatting:
```{r echo = T, results = 'hide', warning=FALSE, message=FALSE}
if (!require(gridExtra)){
  install.packages("gridExtra", dependencies = TRUE)
  library(gridExtra)
}
```

## Context:
A dataset has been chosen with the aim of creating a supervised ML model for classification. We picked a dataset on bird classification by bone structure. There are different types of birds, ranging from flying, to swimming, to running birds. For this dataset, birds have been classified into ecological groups according to their habitats. There are 8 ecological groups of birds:

- Swimming Birds;
- Wading Birds;
- Terrestrial Birds;
- Raptors;
- Scansorial Birds;
- Singing Birds;
- Cursorial Birds (not included in dataset);
- Marine Birds (not included in dataset).

The first 6 groups are covered by this dataset. Birds belonging to different ecological groups have different appearances: flying birds have strong wings and wading birds have long legs. Their living habits are somewhat reflected in their bones' shapes. We may think of examining the underlying relationship between sizes of bones and ecological groups , and recognizing bird's ecological groups by their bone structure.

## Status of data
The dataset comes from Kaggle (https://www.kaggle.com/datasets/zhangjuefei/birds-bones-and-living-habits). It is made up of measurements of bird skeletons from collections of the Natural History Museum of Los Angeles County (data provided by Dr. D. Liu of Beijing Museum of Natural History). It is a 420x10 size continuous values unbalanced multi-class dataset, meaning there are 420 birds contained in this dataset, each bird is represented by 10 measurements (features):

- Huml and Humw: Length and Width of Humerus;
- Ulnal and Ulnaw: Length and Width of Ulna;
- Feml and Femw: Length and Width of Femur;
- Tibl and Tilw: Length and Width of Tibiotarsus;
- Tarl and Tarw: Length and Width of Tarsometatarsus.

The class attribute is "type", referring to the ecological group. The bird skeletons belong to 21 orders, 153 genera, 245 species. Each bird has a label for its ecological group:

- SW: Swimming Birds;
- W: Wading Birds;
- T: Terrestrial Birds;
- R: Raptors;
- P: Scansorial Birds;
- SO: Singing Birds.

## Data exploration
We will load our data into R, making sure to separate by commas. 

```{r}
#load the data
bird_data <- read.table('bird.csv', sep=",", header = 1)
```

Firstly, we need to have an overview of our data. Pictured below is a summary of the dataset:

```{r}
bird_data %>% 
  select (huml, humw, ulnal, ulnaw, feml, femw, tibl, tibw, tarl, 
          tarw, type) -> overview
print(dfSummary(overview, graph.magnif = .75, method = "render"))
```

There are 15 missing values. Let's see which birds those values are attached to;
```{r}
#checking which types of birds have missing data
missing_data_type <- bird_data$type[apply(is.na(bird_data), 1, any)]
missing_data_type
```

So there are a total of 7 birds with missing values. This means there are a total 420 entries with 15 missing values spread across 7 birds. The missing values are the measurements of certain bones but none of the main attribute data; "type", is missing. Apparently there are a few birds with "missing bones". This means a choice must be made to discard the missing data or perform imputation, which means replacing the missing data with substitutions. However 7 out of 420 means the observations with missing values account for less than 3% of the dataset, and may be insignificant enough to remove entirely. Let's also think about why the data is missing; is it significant to specific types of birds or is the data quite random? 

These missing values are most likely field data that was not properly formatted into the csv file. Since the SO (songbirds) are over represented in the data, it seems fine to remove the entries with missing values. The missing values also have no real meaning, since every bird is supposed to have the same amount of bones, which means the missing values are just bones that were not measured.

## Cleaning dataset
The missing values and the id column shall be removed and a new CSV file will be created. This will be the clean dataset with which we will perform machine learning. 
```{r echo = T, results = 'hide', warning=FALSE, message=FALSE}
#remove the ID's and NA's
clean_bird_data <- na.omit(bird_data)
clean_bird_data <- subset(clean_bird_data, select = -id)
#check if any missing values are left
colSums(is.na(clean_bird_data))
#write the csv file with cleaned dataset. 
write.csv(clean_bird_data, "bird_clean.csv", row.names = F)
```

All the missing values have been removed and the cleaned data can now be used for further analysis.

## Class distribution 
We need to check whether or not the data is equally distributed between the types of birds. 
```{r}
#create a barchart per species
ggplot(clean_bird_data, aes(x = type)) + geom_bar(fill = "black") + 
  labs(title = "distribution of bird species", x = "species", y = "count")
```

On a glance we can already see that the distribution is not equal, it is seemingly all over the place. There is an over representation of songbirds (SO) and Swimming birds (SW). Terrestrial birds (T) are the group with the least instances. Wading birds (W), Raptos (R) and Scansorial birds (P) are somewhere in between. The 6 classes are not equally distributed. To remedy this, we may double the lowest group or half the biggest group if the training models are insufficiently trained. For now though, we shall use the dataset as is without any further changes. 

## Variation and distributions 
To check the distribution of the data we can make boxplots of bone types for each type of bird. We want to look for outliers in each type. 
```{r warning=FALSE}
#boxplot of all bonemeasurements
p1 <- ggplot(clean_bird_data, aes(x = type, y = huml)) + geom_boxplot() 
p2 <- ggplot(clean_bird_data, aes(x = type, y = humw)) + geom_boxplot() 
p3 <- ggplot(clean_bird_data, aes(x = type, y = ulnal)) + geom_boxplot() 
p4 <- ggplot(clean_bird_data, aes(x = type, y = ulnaw)) + geom_boxplot()
p5 <- ggplot(clean_bird_data, aes(x = type, y = feml)) + geom_boxplot() 
p6 <- ggplot(clean_bird_data, aes(x = type, y = femw)) + geom_boxplot() 
p7 <- ggplot(clean_bird_data, aes(x = type, y = tibl)) + geom_boxplot()
p8 <- ggplot(clean_bird_data, aes(x = type, y = tibw)) + geom_boxplot() 
p9 <- ggplot(clean_bird_data, aes(x = type, y = tarl)) + geom_boxplot()
p10 <- ggplot(clean_bird_data, aes(x = type, y = tarw)) + geom_boxplot()
grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, 
             p10, nrow = 3, top = "Bone size per type of bird")
```

The data seems to have a lot of outliers, to see whether or not this is relevant we can do a log transformation to check the significance. Usually this will not be done 

```{r warning=FALSE}
#logtransformed boxplot of all bonemeasurements  
plog1 <- ggplot(clean_bird_data, aes(y = huml,x = type)) + geom_boxplot() + 
  scale_y_continuous(trans = "log10")
plog2 <-ggplot(clean_bird_data, aes(y = humw,x = type)) + geom_boxplot() + 
  scale_y_continuous(trans = "log10")
plog3 <- ggplot(clean_bird_data, aes(y = ulnal,x = type)) + geom_boxplot() + 
  scale_y_continuous(trans = "log10")
plog4 <- ggplot(clean_bird_data, aes(y = ulnaw,x = type)) + geom_boxplot() + 
  scale_y_continuous(trans = "log10")
plog5 <- ggplot(clean_bird_data, aes(y = feml,x = type)) + geom_boxplot() + 
  scale_y_continuous(trans = "log10")
plog6 <- ggplot(clean_bird_data, aes(y = femw,x = type)) + geom_boxplot() + 
  scale_y_continuous(trans = "log10")
plog7 <-ggplot(clean_bird_data, aes(y = tibl,x = type)) + geom_boxplot() + 
  scale_y_continuous(trans = "log10")
plog8 <- ggplot(clean_bird_data, aes(y = tibw,x = type)) + geom_boxplot() + 
  scale_y_continuous(trans = "log10")
plog9 <- ggplot(clean_bird_data, aes(y = tarl,x = type)) + geom_boxplot() + 
  scale_y_continuous(trans = "log10")
plog10 <-ggplot(clean_bird_data, aes(y = tarw,x = type)) + geom_boxplot() + 
  scale_y_continuous(trans = "log10")
grid.arrange(plog1, plog2, plog3, plog4, plog5, plog6, plog7, plog8, plog9,
             plog10, nrow = 3, 
             top = "Logtransformed bone size per type of bird")
```

Most outliers of the SW group have been removed entirely. There are still plenty of outliers in T group, but since it's the smallest group no instances shall be removed. It is safe to assume that the outliers are actually just bigger and smaller birds. From now on we can assume the outliers are not significant enough to be removed and thus the regular dataset will be used for machine learning. This data can be used for trees, but if these algorithms prove insufficient, logtransformed data may be used for Naive Bayes.

How can we see if a variable is informative for machine learning? To find out, we will perform an ANOVA on each bone type per bird type and sort the p-values. This will help us determine the most useful values for prediction, even though the distribution it is scewed. (can use t-values as well to order it, highest difference tells us....what?)

```{r}
# Group the data by Species and select numeric columns

```

!!!is een variable informatief voor ml? zoek het uit; annova doen op elke bot type PER type vogel. Op p-waarden sorteren. helpful voor trees. Ordenen bij meest en minst verschillende waardes. 

## Correlation 
Before checking, the logical assumption would be that the data is highly correlated, since all birds have similar bone structures and the birds within a specific type are going to look even more similar. To be sure, however, we can use a scatterplot to visualize the distribution of the bone sizes and the relationship between them. 

```{r, figures-side, fig.show="hold", out.width="50%", warning=FALSE, message=FALSE}
#create a scatterplot of the bone lengths and widths. 
ggplot(clean_bird_data,aes(x=huml,y=humw,color=type))+geom_point()+
  geom_smooth()
ggplot(clean_bird_data,aes(x=ulnal,y=ulnaw,color=type))+geom_point()+
  geom_smooth()
ggplot(clean_bird_data,aes(x=feml,y=femw,color=type))+geom_point()+
  geom_smooth()
ggplot(clean_bird_data,aes(x=tibl,y=tibw,color=type))+geom_point()+
  geom_smooth()
ggplot(clean_bird_data,aes(x=tarl,y=tarw,color=type))+geom_point()+
  geom_smooth()
```
The scatterplot suggest that there is a correlation between bird types and their bone structures. We can also look at a correlation matrix to see precisely how highly correlated our data is:

```{r, warning=FALSE}
ggcorr(clean_bird_data, label = TRUE)
```

Unsurprisingly the correlation matrix also gave us a large positive correlation between all our measurements. This is of course because when birds get bigger, so do their bone measurements. The swimming birds (SW), however, seem a lot bigger than the others. It makes sense that SW has a lot of different measurements because big birds, like swans, and small birds, like coots, are both classified as swimming birds. Since the correlation between different bone measurements is so high, classification could prove difficult. We need to keep this in mind when choosing our model. 

## Research Question 
Based on the exploratory data analysis the research question will be: Using machine learning algorithms, which bone measurements are most informative for classifying birds into ecological groups?

Which then follows that the null hypothesis will be: There is no significant relationship between bird bone measurements and their ecological groups, and bone measurements are not informative for accurate classification. 

And so the alternative hypothesis: There is a significant relationship between bird bone measurements and their ecological groups, and bone measurements are informative for accurate classification.

# Week 4

## Quality metrics 
To assess the performance of our algorithm there are a few metrics that are commonly used: 
- Accuracy/error rate: measures the ratio of correct/incorrect prediction to the total number of predictions, giving us an idea how often the model is correct;
- Precision/recall: measures the accuracy of positive predictions. Important to determine true positives and false positives rate;
- F1 score: mean of precision/recall. Useful for we want to give equal consideration to both true positives and false negatives;
- Specificity: measures true negative rate. Used when true negatives are crucial to know.
- Confusion matrix: used to present a breakdown of true positive, true negative, false positive and false negative counts. 
- ROC curve: A curve showing models performance across thresholds. Helps visualize trade-off between true positive rate and false positive rate.
- speed: the speed with which an algorithm can make predictions. 

We will use a confusion matrix to showcase our most important metrics. The one metric that seems specifically useful for unbalanced datasets is F1 score, this will be shown separately. Speed is not a metric that is important for this relatively low amount of data. We are also uninterested in specificity, because true negatives are not a crucial part of knowing what bone measurements are effective for determining species type. 

## Machine learning in wekka
Considering our dataset is highly imbalanced we can already guess that algorithms that don't rely on balancing, like trees or boosting algorithms, will perform better than those that do. Regardless, the selection of machine learning models has been made to cover to include representatives of all classifier categories:

- ZeroR (baseline measurement)
- oneR (baseline measurement)
- J48 
- Naive Bayes
- SMO
- K-Nearest Neighbor (IBK)
- Simple Logistic Regression
- Random forest

First we will compare all the algorithms and their default values in Wekka with the clean dataset using Wekka experimenter. 

```{r}
wekka_default <- read.csv("wekka_exp_default.csv", header=T)
results_default <- aggregate(wekka_default$Percent_correct, list(wekka_default$Key_Scheme), mean)
colnames(results_default) <- c("algorithm", "percentage correct")
results_default
```
All the chosen algorithms scored higher than ZeroR and OneR. The most promising results come from the K-Nearest Neighbor (89.16% correct), Simple logistic (85.24% correct) and the Random Forest (84.54% correct) algorithm. The hyper parameters shall be adjusted to see if the accuracy can be heightened. KNN scoring above RandomForest is suspicious, especially since the data is highly correlated. This means KNN might be overfitted on default values, so we shall adjust that. All other algorithms below 70% will be removed. 

Adjustments KNN:
- set KNN to 5 instead of 1
- distanceWeighing = weight by 1/distance instead. 
Adjustments J48:
- set tree to unpruned = True
Adjustments SMO:
- linear regression was used as calibrator. 

The results are as follows:
```{r}
#wekka_adjust <- read.csv("wekka_exp_adjusted.arff", header=T, comment.char = "@")
#colnames()
#results_adjust <- aggregate(wekka_adjust$Percent_correct, list(wekka_adjust$Key_Scheme), mean)
#colnames(results_adjust) <- c("algorithm", "percentage correct")
#results_adjust
```

SimpleLogistic and RandomForest come out on top again. The KNN algorithm and J48 tree have recieved a significant decreased performance.  SMO has remained the same. The algorithms that will be used for further analysis are KNN and RandomForest.

With the wekka experimenter, in the wekka classifier, we checked for overfitting using 10-fold crossvalidation. 
-input:clean_bird_dataset
- 100 repetitions 
- CVParameterSelection
- CVparameters(input: K 1.0 10.0 10.0)

=== Classifier model (full training set) ===

Cross-validated Parameter selection.
Classifier: weka.classifiers.lazy.IBk
Cross-validation Parameter: '-K' ranged from 1.0 to 10.0 with 10.0 steps

Classifier Options: -K 1 -W 0 -A "weka.core.neighboursearch.LinearNNSearch -A \"weka.core.EuclideanDistance -R first-last\""

IB1 instance-based classifier
using 1 nearest neighbour(s) for classification


Time taken to build model: 0.1 seconds

=== Stratified cross-validation ===
=== Summary ===

Correctly Classified Instances         366               88.6199 %
Incorrectly Classified Instances        47               11.3801 %
Kappa statistic                          0.854 
Mean absolute error                      0.0417
Root mean squared error                  0.1933
Relative absolute error                 15.9984 %
Root relative squared error             53.5419 %
Total Number of Instances          

Using the experimenter we determined that KNN was not overfitted. Thus this is the model that shall be used. Next we shall determine the most significant attribute to be used with our model. 

# Week 5 

## Attribute selection
Loading in the cleaned dataset, we use the Wekka Explorer to select attributes. We mix and match different search methods with attribute evaluators to find the commonly highest ranking attributes, these will be used in our final model.

=== 1. OneR evaluator + Attribute Ranking settings ===

- Using clean_bird_data as input.
- Search Method: Attribute ranking.

- Attribute Evaluator (supervised, Class (nominal): 11 type): OneR feature evaluator.
- Using 10 fold cross validation for evaluating attributes.
- Minimum bucket size for OneR: 6

```{r table1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "
|Accuracy | Attribute  |
|:-------:|:---------: |
|54.9637  | 1 huml     |
|53.0266  | 4 ulnaw    |
|52.7845  | 2 humw     |
|51.3317  | 3 ulnal    |
|49.6368  | 7 tibl     |
|47.6998  | 10 tarw    |
|47.6998  | 5 feml     |
|46.4891  | 8 tibw     |
|44.7942  | 6 femw     |
|42.8571  | 9 tarl     |
"
cat(tabl) 
```

For the one OneR feature evaluator + ranker method it seems huml and ulnaw is the highest scoring attribute. 

=== 2. CFS subset Evaluator + Exhaustive Search settings ===

- Search Method: Exhaustive Search.
- Start set: no attributes
-	Number of evaluations: 1024
-	Merit of best subset found:    0.427

- Attribute Subset Evaluator (supervised, Class (nominal): 11 type):
- CFS Subset Evaluator
- Including locally predictive attributes
                     
```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl2 <- "
| Attribute  |
|:---------: |
| huml       |
| ulnal      |
| ulnaw      |
| tarw       |
"
cat(tabl2) 
```
- Selected attributes: 1,3,4,10 : 4

For CFS subset evaluator + exhaustive search method again it seems huml and unlal are highest scoring.


=== 3. Wrapper Subset Evaluate + Exhaustive Search ===

- Exhaustive Search.
- Start set: no attributes
- Number of evaluations: 1024
- Merit of best subset found:    0.897

- Attribute Subset Evaluator (supervised, Class (nominal): 11 type):
- Wrapper Subset Evaluator
- Learning scheme: weka.classifiers.lazy.IBk
- Scheme options: -K 1 -W 0 -A weka.core.neighboursearch.LinearNNSearch -A "weka.core.EuclideanDistance -R first-last" 
- Subset evaluation: classification accuracy
- Number of folds for accuracy estimation: 5
- Selected attributes: 1,2,3,5,6,7,9,10 : 8

```{r table3, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl3 <- "
| Attribute  |
|:---------: |
| huml       |
| humw       |
| ulnal      |
| feml       |
| femw       |
| tibl       |
| tarl       |
| tarw       |
"
cat(tabl3) 
```

For Wrapper subset evaluate + exhaustive search it seems huml and humw are the highest scoring. 

Overall it seems the length and width of the humerus and ulna are the most significant measurements for accurate classification of bird groups. These bones are located in the wings of the bird. It makes sense that wingspan would determine the group that the bird belongs to. Lagging behind in ranking are the femur and tibiotarsus bones, which are located in the legs of the bird. All variables however, lie above the 40% accuracy. Which is certainly not insignificant. We will experiment with removing multiple variables from our models and see if the final model's accuracy will improve. 

## ROC curve 
To find the ROC curve we load our dataset into Wekka and go to the explorer, there we can go into the classify menu and use Logistic classifier to visualize the threshold curve. We do this for every attribute. Pictured below are the values for the KNN and the RandomForest algorithm:

## Final model
For our project we have experimented with removing multiple variables from our dataset. Inside the Wekka explorer we removed individual variables, being sure to keep our class variable. We then save all these datasets as .csv files. In the experimenter we check which mix of attributes and algorithms produce the best result. we run 100 repetitions and use 10-fold crossvalidation. In the comparison field we can check the values. 

Settings: 
1. lazy.IBk '-K 1 -W 0 -I -A \"weka.core.neighboursearch.LinearNNSearch -A \\\"weka.core.EuclideanDistance -R first-last\\\"\"' -3080186098777067172
2. trees.RandomForest '-P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' 1116839470751428698

Dataset                   (1) lazy.IBk | (2) trees
--------------------------------------------------
bird_huml                (1000   46.99 |   47.02  
bird_huml_ulna           (1000   66.52 |   64.70  
bird_huml_ulna_feml      (1000   82.72 |   81.68  
bird_rm_tarl_femw        (1000   86.91 |   82.81 *
bird_rm_tarl_femw_tibw   (1000   87.87 |   82.86 *
bird_rm_tarl             (1000   88.08 |   84.23 *
bird_clean               (1000   89.08 |   84.47 *
--------------------------------------------------
                               (v/ /*) |   (0/3/4)


!!!!!!!!!!!
The accuracy decreases the more attributes are removed. The only exception being the tarl attibute, for this reason and to speed upu the model, this attribute will be removed from the dataset. KNN scores higher than the RandomForest algorithm. The final model will be KNN with an adusted dataset, having tarl removed. 

